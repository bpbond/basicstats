---
title: "Linear models"
author: "Ben"
date: "11/2/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(20211002)

library(ggplot2)
theme_set(theme_bw())
```

# Linear models and ordinary least squares regression

**Linear regression** models the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables)...the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models.

"Linear" here means _linear in the parameters_, not necessarily that the relationship is a straight line.

```{r echo=FALSE, fig.height=3, fig.width=5}
d <- data.frame(x = 1:4, y = c(6, 5, 7, 10))
m <- lm(y ~ x, data = d)
d$pred <- predict(m)
ggplot(d, aes(x, y)) + 
  geom_segment(aes(xend = x, yend = pred), size = 1.5, color = "green") +
  geom_point(color = "red", size = 4) +
  geom_line(aes(y = pred), size = 2, color = "blue")
```

In linear regression, the observations (red) are assumed to be the result of random deviations (green) from an underlying relationship (blue) between a dependent variable (y) and an independent variable (x).

Linear regression models are often fitted using the _least squares_ approach, but they may also be fitted in other ways.

**Ordinary least squares** (OLS) is a method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by minimizing the _sum of the squares_ of the differences between the dependent variable and those predicted by the linear function of the independent variable (i.e., the green lines in the figure above). Mathematically, this can be expressed by a relatively simple formula and solved analytically.

# Fitting and assessing linear models

```{r fake}
# Generate some fake data
N <- 15
Aslope <- 1
Bslope <- 3
Aintercept <- 0
Bintercept <- -3
Nseq <- seq_len(N)
Adat <- data.frame(group = "A",
                   x = Nseq,
                   y = Nseq * Aslope + Aintercept)
Bdat <- data.frame(group = "B",
                   x = Nseq,
                   y = Nseq * Bslope + Bintercept)
dat <- rbind(Adat, Bdat)

# Add some random noise
dat$y <- dat$y + rnorm(nrow(dat)) * 2

ggplot(dat, aes(x, y, color = group)) + geom_point(size = 4)
```

## Simplest model (no groups)

```{r m1}
m1 <- lm(y ~ x, data = dat)
summary(m1)
```

The model has an intercept of **`r round(coefficients(m1)[1], 3)`**, which is _not_ significantly different from zero (as the p-value is `r round(summary(m1)$coefficients[1, 4], 3)`).

The model's best-fit slope is **`r round(coefficients(m1)[2], 3)`**.
The p-value for the slope is very small, strong evidence *against* the "null
hypothesis" that there is _no_ relationship between _x_ and _y_. Because there's only a single term in the model, the p-value for the slope is exactly the same as the p-value for the overall model, at the bottom. 

In other words, there's a strong and statistically significant correlation 
between these two variables, explaining about 
`r round(summary(m1)$adj.r.squared * 100, 0)`% (from the adjusted R^2^) 
of the variability in _y_.

```{r m1-plot}
dat$m1 <- predict(m1)
ggplot(dat, aes(x, y, color = group)) +
  geom_point(size = 4) +
  geom_line(aes(y = m1), linetype = 2, color = "black")
```

Unfortunately, if we plot the **model residuals** (the difference between the _predicted_ values and the _actual_ y values), it's obvious that this model is not satisfactory. 

```{r, message=FALSE}
ggplot(dat, aes(m1, y - m1, color = group)) + 
  xlab(expression(Predicted~value~(hat(y)))) + 
  ylab(expression(Residual~(y-hat(y)))) +
  geom_point(size = 4) + 
  geom_smooth(aes(group = 1), se = FALSE)
```

(The blue loess smoother simply makes it easier to see trends.)

**Note:** We can also just use the `plot` function to make a residual plot, as well as visualize other model diagnostics:

```{r}
plot(m1, which = 1)
```

The problem is that the residuals of a linear model should be **homoschedastic**: [normally distributed](https://en.wikipedia.org/wiki/Normal_distribution) with a mean of zero and roughly constant variance. We can test for this using a chi-squared (Ï‡^2^) test:

```{r, message=FALSE}
library(olsrr)
ols_test_breusch_pagan(m1)
```

So this model has **heteroschedastic** residuals; more specifically, in tends to overpredict group `A` and underpredict group `B`.

We need to do better!

## Group as a factor that affects intercept only

We know we have two groups in our data, `A` and `B`; maybe observations from these groups are systematically different in some way, i.e. there's a constant difference between them?

```{r m2}
m2 <- lm(y ~ x + group, data = dat)
summary(m2)
```

Let's take a close look at this fitted model.

```{r, echo=FALSE}
sm2 <- summary(m2)$coefficients
```

The model has an overall intercept of **`r round(sm2[1, 1], 3)`**, which is significantly different from zero (as the p-value is `r round(sm2[1, 4], 3)`). **This is the intercept for the `A` group.**

The intercept for the `B` group is `r round(sm2[1, 1], 3)` + `r round(sm2[3, 1], 3)` = **`r round(sm2[1, 1] + sm2[3, 1], 3)`**. This is significantly different from the `A` group, because the p-value for `groupB` is **`r sm2[3, 4]`** (i.e. it's very small).

The model's slope is **`r round(sm2[2, 1], 3)`**.
The p-value for the slope is very small, strong evidence *against* the "null
hypothesis" that there is _no_ overall relationship between _x_ and _y_. In other words, the slope is statistically different from zero.

Remember, in our model specification above we did not allow `group` to affect the slope parameter.

We've made progress! This new model explains 
`r round(summary(m2)$adj.r.squared * 100, 0)`% of the variability in _y_, a huge improvement, and has a much lower error.

Visualizing the fitted model and its residuals:

```{r echo=FALSE, fig.show="hold", message=FALSE, out.width="50%"}
dat$m2 <- predict(m2)
ggplot(dat, aes(x, y, color = group)) + 
  geom_point(size = 4) +
  geom_line(aes(y = m2), linetype = 2)

ggplot(dat, aes(m2, y - m2, color = group)) + 
  xlab(expression(Predicted~value~(hat(y)))) + 
  ylab(expression(Residual~(y-hat(y)))) +
  geom_point(size = 4) + 
  geom_smooth(aes(group = 1), se = FALSE)
#plot(m2, which = 1)
```

The residual plot is still pretty weird looking, because we're underpredicting `A` (and overpredicting `B`) at low `x` values, and overpredicting `A` (and underpredicting `B`) at high `x`.

## Group as a factor that affects slope only

If we allow `group` to _interact_ with _x_, then we are fitting a model in which slope (but not intercept) changes with `group`: 

```{r m3}
m3 <- lm(y ~ x : group, data = dat)
summary(m3)
```

```{r, echo=FALSE}
sm3 <- summary(m3)$coefficients
```

The model has a single intercept of **`r round(sm3[1, 1], 3)`**, which is _not_ significantly different from zero.

The best-fit slope for the `A` group is **`r round(sm2[2, 1], 3)`**. The slope for the `B` group is `r round(sm3[2, 1], 3)` + `r round(sm3[3, 1], 3)` = **`r round(sm3[2, 1] + sm3[3, 1], 3)`**. This is significantly different from the `A` group, because the p-value for `x:groupB` is **`r sm2[3, 4]`**.

This third model explains 
`r round(summary(m3)$adj.r.squared * 100, 0)`% of the variability in _y_.

```{r, echo=FALSE, message=FALSE, message=FALSE, out.width="50%"}
dat$m3 <- predict(m3)
ggplot(dat, aes(x, y, color = group)) + 
  geom_point(size = 4) +
  geom_line(aes(y = m3), linetype = 2)

ggplot(dat, aes(m3, y - m3, color = group)) + 
  xlab(expression(Predicted~value~(hat(y)))) + 
  ylab(expression(Residual~(y-hat(y)))) +
  geom_point(size = 4) + 
  geom_smooth(aes(group = 1), se = FALSE)
#plot(m3, which = 1)
```

This...this is a pretty good looking model. It explains almost all the variability in the data, and the residual plot looks good.

## Group as a factor affecting both slope and intercept

Should we try a model with `group` affecting _both_ slope and intercept? **It depends on our understanding of the system we're trying to model.** Does it make sense for these models to have a common intercept? In many cases, probably not: the groups would be expected to be completely different. 

```{r m4}
m4 <- lm(y ~ x * group, data = dat)
summary(m4)
```

```{r, echo=FALSE}
sm4 <- summary(m4)$coefficients
```

Our final model. How did OLS do?

Term        | True value     | Estimate
----------- | -------------- | ---------
A intercept | `r Aintercept` | `r round(sm4[1, 1], 3)`
B intercept | `r Bintercept` | `r round(sm4[1, 1], 3)` + `r round(sm4[3, 1], 3)` = `r round(sm4[1, 1] + sm4[3, 1], 3)`
A slope     | `r Aslope`     | `r round(sm4[2, 1], 3)`
B slope     | `r Bslope`     | `r round(sm4[2, 1], 3)` + `r round(sm4[4, 1], 3)` = `r round(sm4[2, 1] + sm4[4, 1], 3)`

```{r, echo=FALSE, message=FALSE, message=FALSE, out.width="50%"}
dat$m4 <- predict(m4)
ggplot(dat, aes(x, y, color = group)) + 
  geom_point(size = 4) +
  geom_line(aes(y = m4), linetype = 2)

ggplot(dat, aes(m4, y - m4, color = group)) + 
  xlab(expression(Predicted~value~(hat(y)))) + 
  ylab(expression(Residual~(y-hat(y)))) +
  geom_point(size = 4) + 
  geom_smooth(aes(group = 1), se = FALSE)
#plot(m4, which = 1)
```

## Transformations

Some problems--in particular, heteroschedasticity--may be addressable by _transforming_ the data so that we can model it successfully using a linear model.

For example, look at the `Puromycin` dataset and fit a basic one-term linear model. Obviously it doesn't work very well:

```{r, echo=FALSE, message=FALSE, fig.show="hold", out.width="50%"}
pm1 <- lm(rate ~ conc * state, data = Puromycin)
Puromycin$pm1 <- predict(pm1, data = Puromycin)
ggplot(Puromycin, aes(conc, rate, color = state)) + 
  geom_point(size = 4) + geom_line(aes(y = pm1), linetype = 2)
plot(pm1, which = 1)
```

Use a polynomial:


```{r}
pm2 <- lm(rate ~ poly(conc, 2) * state, data = Puromycin)
```

```{r puromycin-poly, echo=FALSE, message=FALSE, fig.show="hold", out.width="50%"}
Puromycin$pm2 <- predict(pm2, data = Puromycin)
ggplot(Puromycin, aes(conc, rate, color = state)) + 
  geom_point(size = 4) + geom_line(aes(y = pm2), linetype = 2)
plot(pm2, which = 1)
```

Use a transformation:

```{r}
pm3 <- lm(rate ~ log(conc) * state, data = Puromycin)
```

```{r puromycin-trans, echo=FALSE, message=FALSE, fig.show="hold", out.width="50%"}
Puromycin$pm3 <- predict(pm3, data = Puromycin)
ggplot(Puromycin, aes(conc, rate, color = state)) + 
  geom_point(size = 4) + geom_line(aes(y = pm3), linetype = 2)
plot(pm3, which = 1)
```

The other, and probably best, option would be to use a _nonlinear_ model.
