---
title: "linear"
author: "Ben"
date: "11/2/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(20211002)
```

# Ordinary least squares regression and linear models

* What is OLS?
* What is a "linear model"
* Error

## Linear models

```{r fake}
# Generate some fake data
N <- 15
Aslope <- 1
Bslope <- 3
Aintercept <- 0
Bintercept <- -3
Nseq <- seq_len(N)
dat1 <- data.frame(group = "A",
                   x = Nseq,
                   y = Nseq * Aslope + rnorm(N) * 2 + Aintercept)
dat2 <- data.frame(group = "B",
                   x = Nseq,
                   y = Nseq * Bslope + rnorm(N) * 2 + Bintercept)

dat <- rbind(dat1, dat2)

library(ggplot2)
theme_set(theme_bw())
ggplot(dat, aes(x, y, color = group)) + geom_point(size = 4)
```

## Simplest model (no groups)

```{r m1}
m1 <- lm(y ~ x, data = dat)
summary(m1)
```

The model has an intercept of **`r round(coefficients(m1)[1], 3)`**, which is _not_ significantly different from zero (as the p-value is `r round(summary(m1)$coefficients[1, 4], 3)`).

The model's best-fit slope is **`r round(coefficients(m1)[2], 3)`**.
The p-value for the slope is very small, strong evidence *against* the "null
hypothesis" that there is _no_ relationship between _x_ and _y_. (Because there's only a single term in the model, the p-value for the slope is exactly the same as the p-value for the overall model, at the bottom.)

In other words, there's a strong and statistically significant correlation 
between these two variables, explaining about 
`r round(summary(m1)$r.squared * 100, 0)`% (from the R^2^) 
of the variability in _y_.

```{r m1-plot}
dat$m1 <- predict(m1)
ggplot(dat, aes(x, y, color = group)) + geom_point(size = 4) +
  geom_line(aes(y = m1), linetype = 2, color = "black")
```

Unfortunately, if we plot the **model residuals** (along with a loess smoother, in blue, to same it easier to see trends), it's obvious that this model is not satisfactory:

```{r, echo=FALSE, message=FALSE}
ggplot(dat, aes(m1, y - m1, color = group)) + 
  xlab(expression(Predicted~value~(hat(y)))) + 
  ylab(expression(Residual~(y-hat(y)))) +
  geom_point(size = 4) + 
  geom_smooth(aes(group = 1), se = FALSE)
```

The residuals of a linear model should be **homoschedastic**: normally distributed with a mean of zero and roughly constant variance. We can test for this using a chi-squared (Ï‡^2^) test:

```{r, message=FALSE}
library(olsrr)
ols_test_breusch_pagan(m1)
```

So this model has **heteroschedastic** residuals. We need to do better!

## Group as a factor but affects intercept only

We know we have two groups in our data, `A` and `B`; maybe observations from these groups are systematically different in some way, i.e. there's a constant difference between them?

```{r m2}
m2 <- lm(y ~ x + group, data = dat)
summary(m2)
```

Let's take a close look at this fitted model. TODO

```{r, echo=FALSE, message=FALSE}
dat$m2 <- predict(m2)
ggplot(dat, aes(x, y, color = group)) + 
  geom_point(size = 4) +
  geom_line(aes(y = m2), linetype = 2)

ggplot(dat, aes(m2, y - m2, color = group)) + 
  xlab(expression(Predicted~value~(hat(y)))) + 
  ylab(expression(Residual~(y-hat(y)))) +
  geom_point(size = 4) + 
  geom_smooth(aes(group = 1), se = FALSE)
```

## Group as a factor but affects slope only

```{r m3}
m3 <- lm(y ~ x : group, data = dat)
summary(m3)
```


```{r, echo=FALSE, message=FALSE}
dat$m3 <- predict(m3)
ggplot(dat, aes(x, y, color = group)) + 
  geom_point(size = 4) +
  geom_line(aes(y = m3), linetype = 2)

ggplot(dat, aes(m3, y - m3, color = group)) + 
  xlab(expression(Predicted~value~(hat(y)))) + 
  ylab(expression(Residual~(y-hat(y)))) +
  geom_point(size = 4) + 
  geom_smooth(aes(group = 1), se = FALSE)
```


## Group as a factor affecting both slope and intercept

```{r m4}
m4 <- lm(y ~ x * group, data = dat)
summary(m4)
```


```{r, echo=FALSE, message=FALSE}
dat$m4 <- predict(m4)
ggplot(dat, aes(x, y, color = group)) + 
  geom_point(size = 4) +
  geom_line(aes(y = m4), linetype = 2)

ggplot(dat, aes(m4, y - m4, color = group)) + 
  xlab(expression(Predicted~value~(hat(y)))) + 
  ylab(expression(Residual~(y-hat(y)))) +
  geom_point(size = 4) + 
  geom_smooth(aes(group = 1), se = FALSE)
```

## Transformations

Some problems--in particular, heteroschedasticity--may be addressable by _transforming_ the data so that we can model it successfully using a linear model.

For example, look at the `Puromycin` dataset and fit a basic one-term linear model. Obviously it doesn't work very well:

```{r, echo=FALSE, message=FALSE, fig.show="hold", out.width="50%"}
pm1 <- lm(rate ~ conc * state, data = Puromycin)
Puromycin$pm1 <- predict(pm1, data = Puromycin)
ggplot(Puromycin, aes(conc, rate, color = state)) + 
  geom_point(size = 4) + geom_line(aes(y = pm1), linetype = 2)
plot(pm1, which = 1)
```

Use a polynomial:


```{r}
pm2 <- lm(rate ~ poly(conc, 2) * state, data = Puromycin)
```

```{r puromycin-poly, echo=FALSE, message=FALSE, fig.show="hold", out.width="50%"}
Puromycin$pm2 <- predict(pm2, data = Puromycin)
ggplot(Puromycin, aes(conc, rate, color = state)) + 
  geom_point(size = 4) + geom_line(aes(y = pm2), linetype = 2)
plot(pm2, which = 1)
```

Use a transformation:

```{r}
pm3 <- lm(rate ~ log(conc) * state, data = Puromycin)
```

```{r puromycin-trans, echo=FALSE, message=FALSE, fig.show="hold", out.width="50%"}
Puromycin$pm3 <- predict(pm3, data = Puromycin)
ggplot(Puromycin, aes(conc, rate, color = state)) + 
  geom_point(size = 4) + geom_line(aes(y = pm3), linetype = 2)
plot(pm3, which = 1)
```

The other, and probably best, option would be to use a _nonlinear_ model.

