---
title: "linear"
author: "Ben"
date: "11/2/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(20211002)
```

## Linear models

```{r fake}
# Generate some fake data
N <- 15
Aslope <- 1
Bslope <- 3
Aintercept <- 0
Bintercept <- -3
Nseq <- seq_len(N)
dat1 <- data.frame(group = "A",
                   x = Nseq,
                   y = Nseq * Aslope + rnorm(N) * 2 + Aintercept)
dat2 <- data.frame(group = "B",
                   x = Nseq,
                   y = Nseq * Bslope + rnorm(N) * 2 + Bintercept)

dat <- rbind(dat1, dat2)

library(ggplot2)
theme_set(theme_bw())
ggplot(dat, aes(x, y, color = group)) + geom_point(size = 4)
```

## Simplest model (no groups)

```{r m1}
m1 <- lm(y ~ x, data = dat)
summary(m1)

dat$m1 <- predict(m1)
ggplot(dat, aes(x, y, color = group)) + geom_point(size = 4) +
  geom_line(aes(y = m1), linetype = 2, color = "black")
```

The model has an intercept of **`r round(coefficients(m1)[1], 3)`**, which is _not_ significantly different from zero (as the p-value is `r round(summary(m1)$coefficients[1, 4], 3)`).

The model's best-fit slope is **`r round(coefficients(m1)[2], 3)`**.
The p-value for the slope is very small, strong evidence *against* the "null
hypothesis" that there is _no_ relationship between _x_ and _y_. 

In other words, there's a strong and statistically significant correlation 
between these two variables, explaining about 
`r round(summary(m1)$r.squared * 100, 0)`% (from the R^2^) 
of the variability in _y_.

Unfortunately, if we plot the **model residuals**, it's obvious that this model is not satisfactory:

```{r, echo=FALSE, message=FALSE}
ggplot(dat, aes(m1, y - m1, color = group)) + 
  xlab(expression(Predicted~value~(hat(y)))) + 
  ylab(expression(Residual~(y-hat(y)))) +
  geom_point(size = 4) + 
  geom_smooth(aes(group = 1), se = FALSE)
```

The residuals of a linear model should be _homoschedastic_: normally distributed with a mean of zero and roughly constant variance. We can test for this using a chi-squared (Ï‡^2^) test:

```{r, message=FALSE}
library(olsrr)
ols_test_breusch_pagan(m1)
```

We need to find a better model!

## Group as a factor but affects intercept only

```{r m2}
m2 <- lm(y ~ x + group, data = dat)
summary(m2)
```


```{r, echo=FALSE, message=FALSE}
dat$m2 <- predict(m2)
ggplot(dat, aes(x, y, color = group)) + 
  geom_point(size = 4) +
  geom_line(aes(y = m2), linetype = 2)

ggplot(dat, aes(m2, y - m2, color = group)) + 
  xlab(expression(Predicted~value~(hat(y)))) + 
  ylab(expression(Residual~(y-hat(y)))) +
  geom_point(size = 4) + 
  geom_smooth(aes(group = 1), se = FALSE)
```

## Group as a factor but affects slope only

```{r m3}
m3 <- lm(y ~ x : group, data = dat)
summary(m3)
```


```{r, echo=FALSE, message=FALSE}
dat$m3 <- predict(m3)
ggplot(dat, aes(x, y, color = group)) + 
  geom_point(size = 4) +
  geom_line(aes(y = m3), linetype = 2)

ggplot(dat, aes(m3, y - m3, color = group)) + 
  xlab(expression(Predicted~value~(hat(y)))) + 
  ylab(expression(Residual~(y-hat(y)))) +
  geom_point(size = 4) + 
  geom_smooth(aes(group = 1), se = FALSE)
```


## Group as a factor affecting both slope and intercept

```{r m4}
m4 <- lm(y ~ x * group, data = dat)
summary(m4)
```


```{r, echo=FALSE, message=FALSE}
dat$m4 <- predict(m4)
ggplot(dat, aes(x, y, color = group)) + 
  geom_point(size = 4) +
  geom_line(aes(y = m4), linetype = 2)

ggplot(dat, aes(m4, y - m4, color = group)) + 
  xlab(expression(Predicted~value~(hat(y)))) + 
  ylab(expression(Residual~(y-hat(y)))) +
  geom_point(size = 4) + 
  geom_smooth(aes(group = 1), se = FALSE)
```


