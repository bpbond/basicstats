---
title: "linear"
author: "Ben"
date: "11/2/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(20210102)
```

## Linear models

```{r fake}
# Generate some fake data
dat1 <- data.frame(group = "A",
                   x = 1:10,
                   y = 1:10 + rnorm(10))
dat2 <- data.frame(group = "B",
                   x = 1:10,
                   y = 1:10 * 2 + rnorm(10) - 3)

dat <- rbind(dat1, dat2)

library(ggplot2)
ggplot(dat, aes(x, y, color = group)) + geom_point()
```

## Simplest model (no groups)

```{r m1}
m1 <- lm(y ~ x, data = dat)
summary(m1)

dat$m1 <- predict(m1)
ggplot(dat, aes(x, y, color = group)) + geom_point() +
  geom_line(aes(y = m1), color = "black")
```

The model has an intercept of **`r round(coefficients(m1)[1], 3)`**, which is _not_ significantly different from zero (as the p-value is `r round(summary(m1)$coefficients[1, 4], 3)`).

The model's best-fit slope is **`r round(coefficients(m1)[2], 3)`**.
The p-value for the slope is very small, strong evidence *against* the "null
hypothesis": that there is no relationship between _x_ and _y_. 

In other words, there's a strong and statistically significant correlation 
between these two variables, explaining about 
`r round(summary(m1)$r.squared * 100, 0)`% (from the R^2^) 
of the variability in _y_.

## Group as a factor but affects intercept only

```{r m2}
m2 <- lm(y ~ x + group, data = dat)
summary(m2)

dat$m2 <- predict(m2)
ggplot(dat, aes(x, y, color = group)) + geom_point() +
  geom_line(aes(y = m2), linetype = 2)
```

## Group as a factor but affects slope only

```{r m3}
m3 <- lm(y ~ x : group, data = dat)
summary(m3)

dat$m3 <- predict(m3)
ggplot(dat, aes(x, y, color = group)) + geom_point() +
  geom_line(aes(y = m3), linetype = 2)
```

## Group as a factor affecting both slope and intercept

```{r m4}
m4 <- lm(y ~ x * group, data = dat)
summary(m4)

dat$m4 <- predict(m4)
ggplot(dat, aes(x, y, color = group)) + geom_point() +
  geom_line(aes(y = m4), linetype = 2)
```

