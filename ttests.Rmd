---
title: "Testing for group differences"
author: "Ben"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(20211002)

library(tibble)
library(tidyr)
library(ggplot2)
theme_set(theme_minimal())
```


# How do we test for group differences?

Consider R's `sleep` dataset:

```{r, eval=FALSE}
?sleep
```

>Data which show the effect of two soporific drugs (increase in hours of sleep compared to control) on 10 patients. A data frame with 20 observations on 3 variables.

```{r plot-sleep}
tapply(sleep$extra, sleep$group, summary)
ggplot(sleep, aes(group, extra)) + 
  geom_boxplot(color = "lightgrey") + 
  geom_point(color = "blue", size = 3)
```

**How do we know if the differences between these groups are _statistically significant?_**

To answer this question, we need to understand:

* Random variables
* The scope of our inference: populations versus samples
* Sample statistics: variance and standard deviation
* The normal distribution
* Hypothesis testing
* The Student's t-test


# Dataset variables

A variable is an _attribute_ of the data. They fall into two categories:

* Quantitative variables are those for which the value has numerical meaning. The value refers to a _specific amount_ of something.
* Categorical variables indicate group membership; this may be inherent (gender is a classic example) or assigned (treatment versus control groups).

A closely related idea is that of _random variables_:

>A random variable is a numerical description of the outcome of a statistical experiment. A random variable that may assume only a finite number or an infinite sequence of values is said to be _discrete_; one that may assume any value in some interval on the real number line is said to be _continuous_.

From [Encyclopedia Brittanica](https://www.britannica.com/science/statistics/Random-variables-and-probability-distributions). 

In the `sleep` dataset:

* `extra` is a numerical variable
* `group` is a categorical variable
* What is `ID`?


# Population vs. Sample

When we conduct a study, we define the _population of interest_. This is the entire set of elements that possess the characteristics of interest. For example, for 
`sleep`, this might be "all people", "all old women" (if all subjects could
be described in this way), "all Maryland high school students who need soporifics",
etc. 

**We generally cannot observe or measure _all_ the 'elements of interest'.**
By definition, populations are large are frequently inaccessible due to time,
money, logistics, etc. And so we randomly sample from the population.

* A _population_ includes all elements of interest; while
* A _sample_ consists of a subset of observations from a population.

This is important, because the nomenclature, statistical formulas, notation, 
and scope of inference vary depending on whether your analyzing a 
population or a sample.


# Mean, variance, and standard deviation

We can compute several _statistics_ from our sample(s) that are both
informative and fundamental to statistical tests.

The _mean_ of the sample is its average:

$$
\huge\bar{x} = \frac{\sum_{i = 1}^{n}{x_i}}{n}
$$
To understand a variable’s distribution, we use measures of variability.
_Variance_ measures the dispersion of the data from the mean:

$$
\huge s^2 = \frac{\sum_{i = 1}^{n}{(x_i - \bar{x})^2}}{n - 1}
$$
That `n - 1` in the denominator constitutes the [degrees of freedom](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)): 
the number of independent pieces of information that go into the estimate
of a test statistic.

Finally, the _standard deviation_ is the square root of the variance:

$$
\huge s = \sqrt{s^2}
$$

By taking the square root, we return the measure to the original units of _x_ 
The standard deviation indicates how close the data are to the mean:

```{r, echo=FALSE}
dat <- tibble()
for(s_val in c(0.5, 1, 2, 4)) {
  dat <- rbind(dat, tibble(s = paste("s =", s_val), 
                           x = rnorm(n = 100, sd = s_val),
                           y = rnorm(n = 100, sd = s_val)))
}
ggplot(dat, aes(x, y, color = s)) + geom_point() + facet_wrap(~s) +
  theme(axis.title = element_blank(),
        axis.text = element_blank(),
        strip.text = element_text(size = 14),
        legend.position = "none")
```


# The central limit theorem

**As our sample size increases, its mean and variance approach that of the 
overall population.** This is due to the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem), 
the core of much of modern statistics.

Watch how the distribution (density) of these random data change as
the sample size increases, getting closer and closer to the 
theoretical _normal distribution_ (dashed line):

```{r, echo=FALSE}
dat <- tibble()
for(ss in c(5, 10, 100, 1000)) {
  dat <- rbind(dat, tibble(ss = ss,
                           x = rnorm(ss)))
}
ggplot(dat, aes(x, color = as.factor(ss))) + 
  geom_density(size = 1) + 
  facet_wrap(~ss) + 
  stat_function(fun = dnorm, linetype = 2) +
  theme(axis.title = element_blank(),
        axis.text = element_blank(),
        strip.text = element_text(size = 14),
        legend.position = "none")
```


# The normal distribution of a _population_

Many statistical tests assume a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution), 
meaning that the data approximate a bell-shaped curve. 
In normal distributions, 68% of the data fall within ±1 standard deviation 
from the mean; 95% within 2 standard deviations, and 99% within 3.

```{r normal, echo=FALSE}
add_sd_lines <- function(p, n_sd, ht, clr, lbl) {
  p + annotate(geom = "segment", x = -n_sd, y = ht, xend = n_sd, yend = ht, 
               arrow = arrow(ends = "both"), size = 0.75, color = clr) +
    geom_vline(xintercept = c(-n_sd, n_sd), color = clr, linetype = 2) +
    geom_label(x = 0, y = ht, label = lbl, color = clr)
}

distribution_graph_theme <- function() {
  theme_minimal() %+replace%
    theme(axis.text.y = element_blank(),
          panel.grid.major.y = element_blank(),
          panel.grid.minor.y = element_blank(),
          axis.text.x = element_text(face = "bold"),
          axis.title.x = element_text(size = 12, face = "bold"))
}

p <- ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, size = 1) +
  distribution_graph_theme() +
  labs(title = expression(paste(italic(N), "(", mu, " = 0, ", sigma," = 1)")),
       x = "Standard deviations from the mean", y = "")
p <- add_sd_lines(p, 1, 0.2, "lightblue", "68.2%")
p <- add_sd_lines(p, 2, 0.15, "blue", "95.4%")
p <- add_sd_lines(p, 3, 0.1, "darkblue", "99.7%")
p
```


# Student's t distribution

From [Wikipedia](https://en.wikipedia.org/wiki/Student%27s_t-distribution):

>In probability and statistics, Student's t-distribution (or simply the t-distribution) is any member of a family of continuous probability distributions that arise when estimating the mean of a **normally distributed population** in situations where the **sample size is small** and the population's **standard deviation is unknown**.

```{r students, echo=FALSE}
dat <- data.frame()
for(df in c(1, 2, 5, 10, 20)) {
  dat <- rbind(dat, tibble(x = seq(-4, 4, by = 0.05), 
                           y = dt(x, df = df),
                           df = df))
}
dat$df <- as.factor(dat$df)
ggplot(dat, aes(x, y)) + geom_line(aes(color = df)) +
  scale_color_discrete("Degrees of\nfreedom") +
  stat_function(fun = dnorm, linetype = 2) +
  distribution_graph_theme() +
  theme(legend.position = c(0.8, 0.7)) +
  labs(x = "Standard deviations from the mean", y = "") +
  annotate("text", x = -2, y = 0.3, label = "Dashed line\nshows normal\ndistribution", size = 3)
```


# Hypothesis testing

Hypothesis testing is the process by which we reject or fail to reject statistical hypotheses. The two types of statistical hypotheses:

• The null hypothesis, $H_0$, is the hypothesis that the result from the statistical analysis occurs purely by chance.
• The alternative hypothesis, $H_1$ or $H_a$, is the hypothesis that the result is meaningful and not due to chance.

**The result of the analysis is always used to test the null hypothesis.** 
This means, in science, that we never "prove" hypotheses, we only "disprove" them.

To test a statistical hypothesis, you:

1. **State the null and alternative hypotheses**, in such a way that they are mutually exclusive. The null hypothesis is usually the opposite of the result that you hope to find.
2. **Select the significance level.** This value (for example, 0.01, 0.05, or 0.10) represents the probability of obtaining a significant result if the null hypothesis is actually true...meaning, the chance that you rejected the null hypothesis when you should have failed to reject it (a "false positive").
3. **Determine which statistical analysis to conduct.** 
4. **Analyze the data** by calculating the _test statistic_ and determining the probability of that statistic. Is that probability larger than the significance level you selected, you fail to reject the null hypothesis; if it’s smaller, you do reject it.


# The Student's t test

Time to bring together all these concepts!

A **t-test** measures the difference in group means divided by the pooled standard 
error of the two group means. It then computes, based on the Student's t-distribution, how _likely_ it would be to observe that difference if there is 
actually no difference between the groups.

It is a [parametric](https://en.wikipedia.org/wiki/Parametric_statistics)
test of difference, meaning that it assumes your data:

* are independent;
* are (approximately) normally distributed; and
* have a similar amount of variance within each group being compared.

```{r}
group1 <- sleep[sleep$group == 1,]$extra
group2 <- sleep[sleep$group == 2,]$extra
t.test(group1, group2)
```

`t.test` helpfully prints out

* the alternative (as opposed to the _null_) hypothesis: "true difference in means is not equal to 0";
* the pooled _degrees of freedom_ (closely related to the sample size); and
* the _p-value_. Here it is 0.079, which traditionally would be interpreted as _no or weak evidence_ against the null hypothesis (because our significance level--see above--is by default 0.05).

We might have a narrower null hypothesis, however, for example that group 2's
mean is greater than that of group 1:

```{r}
t.test(group1, group2, alternative = "less")
```

Traditionally, this would be interpreted as _moderate evidence_ against
the null hypothesis.


# Paired versus unpaired

**But wait!** Let's look again at `sleep` (only showing a few rows for clarity):

```{r, echo=FALSE}
sleep[sleep$ID %in% c(1, 2, 9, 10),] %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling(full_width = FALSE)
```

`sleep` **does not consist of two random samples drawn from a population**;
rather, it is 10 random subjects (the `ID` column; see the help page) 
_measured twice_, once for each drug. Visually:

```{r plot-paired-sleep, echo=FALSE}
sleep2 <- pivot_wider(sleep, names_from =  "group", values_from = "extra")

ggplot(sleep, aes(group, extra)) + 
  geom_boxplot(color = "lightgrey") + 
  geom_point(color = "blue", size = 3) + 
  geom_segment(data = sleep2, x = 1, xend = 2, aes(y = `1`, yend = `2`),
               color = "blue", linetype = 2, size = 0.25)
```

This means that we can use a **paired t-test**, which is more powerful than
the unpaired t-test because the pairing of samples reduces inter-subject 
variability (as it makes comparisons between the same subject).

```{r}
t.test(group1, group2, paired = TRUE)
```

This is _strong evidence_ against the null hypothesis: we would observe 
such data only about 0.28% of the time if there's actually no difference 
between the two drugs.


# The End

Much of the structure, and some of the language, in this file follows Microsoft's
[Statistics Primer: A Brief Overview of Basic Statistical and Probability Principles](https://orla.osd.wednet.edu/common/pages/UserFile.aspx?fileId=30749993).

The repository for this document is [here](https://github.com/bpbond/linear).

```{r, echo=FALSE}
sessionInfo()
```
